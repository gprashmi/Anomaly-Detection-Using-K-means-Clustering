{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection using K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "#ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data using standard input(stdin) and put the data into a pandas dataframe\n",
    "#2D data\n",
    "dataset = pd.read_csv('t11.dat',header=None)\n",
    "#1D data\n",
    "#dataset = pd.read_csv('t10.dat',header=None)\n",
    "#Illegal data\n",
    "#dataset = pd.read_csv('t19.dat',header=None)\n",
    "#to work with any data from command line\n",
    "#dataset = pd.read_csv(sys.stdin,header=None)\n",
    "\n",
    "#Find the shape of the dataset.\n",
    "#Samples are the number of points and feature is the dimension\n",
    "samples, features = dataset.shape\n",
    "\n",
    "#Convert the dataframe to an array,data\n",
    "data = dataset.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Canopy Method\n",
    "\n",
    "Functionality: This method is used to find the farthest centriods from each other based on their distance.\n",
    "Step1: A random,initial centroid from the dataset is chosen using the random.choice fucntion\n",
    "Step2: Using the library function scipy, cdist with metrics as elucidean distance is calculated between initial centroid and all data points.\n",
    "Step3: Find the maximum distance and the corresponding data point. This becomes the next centroid\n",
    "Step4: A while loop is run to repeat step3 till the number of centroids determined equals the number of clusters,k. It the centroid gets repeated, the distance and the corresponding\n",
    "       datapoint is deleted and the next maximum distance and its point is the centroid\n",
    "Step5: Inside the Step4 while loop, there is another while loop to keep track if no repeating centroids are appendend to the final \"centroids\" list\n",
    "\n",
    "Return value: List of centroids whose length equals the number of clusters,k\n",
    "'''\n",
    "\n",
    "def canopy(data,features,clu):\n",
    "    initial_centroid = []   \n",
    "    i = 1\n",
    "    distances = 0\n",
    "    centroids = []\n",
    "    #select a random centroid from the dataset\n",
    "    initial_centroid.append(np.reshape(random.choice(data),(1,features)).tolist())\n",
    "\n",
    "    #find the other centroids based on cluster, clu value\n",
    "    while i < clu:\n",
    "            \n",
    "        data1 = copy.deepcopy(data)\n",
    "\n",
    "        for j in initial_centroid:\n",
    "            #sum the distances of all data points to a centroid\n",
    "            distances += distance.cdist(j,data,metric = \"euclidean\")\n",
    "            \n",
    "        distnce = distances.flatten().tolist()\n",
    "        max_dist = max(distnce)\n",
    "\n",
    "        #keep track of repeating centroids\n",
    "        while [data1[distnce.index(max_dist)]] in initial_centroid:\n",
    "            del data1[distnce.index(max_dist)]\n",
    "            del distnce[distnce.index(max_dist)]\n",
    "            max_dist = max(distnce)\n",
    "\n",
    "        #append the maximum distance point and next centroid\n",
    "        initial_centroid.append((np.reshape(data1[distnce.index(max_dist)],(-1,features))).tolist())\n",
    "\n",
    "        #set distances back to zero for next cluster, k\n",
    "        distances = 0\n",
    "        i += 1\n",
    "\n",
    "    #copy intial_centroids list to centroids list\n",
    "    centroids = copy.deepcopy(initial_centroid)\n",
    "    \n",
    "    #intialise initial_centroid and i back to original values for next cluster iteration\n",
    "    initial_centroid = []\n",
    "    i = 1\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "K-Means assignment function\n",
    "\n",
    "Functionality: The distance between each datapoint and the a centroid is calculated using scipy.cdist and added as a column to the dataset DF. The minimum distance to a\n",
    "given point is determined and is assigned to the corresponding cluster.\n",
    "\n",
    "Return Value: A dataframe with clusters column updated with cluster numbers to all the data points, dictionary with cluster and their points which is used for the\n",
    "kmeans_update function, original dataframe\n",
    "\n",
    "'''\n",
    "def kmeans_assign(centroids,data,dataset,features):\n",
    "    \n",
    "    #find the distance of the data points to the each centroids\n",
    "    for cen in range(len(centroids)):\n",
    "        \n",
    "        #create a column in the dataset df with the distances of data points from all the centroids\n",
    "        #flatten() and tolist() are used to convert the distance numpy array generated and append to the column\n",
    "        dataset[\"{}\".format(cen)] = distance.cdist(centroids[cen],data,metric = \"euclidean\").flatten().tolist()\n",
    "    \n",
    "    #create the clusters column in the dataset df to assig the cluster\n",
    "    #using idxmin(axis = 1), minimun distance is determined and corresponding column name is assigned in the cluster column\n",
    "    dataset[\"clusters\"] = dataset.iloc[:,features:].astype(float).idxmin(axis = 1)\n",
    "\n",
    "    #create a dictionary of datapoints as values and their cluster as keys\n",
    "    data_dict = {}\n",
    "\n",
    "    for clustr in dataset['clusters'].unique():        \n",
    "        data_dict[clustr] = [[dataset[fea][y] for fea in range(0,features)] for y in dataset[dataset['clusters'] == clustr].index]\n",
    "\n",
    "    #copy the dataset to a final_df\n",
    "    final_dataset = copy.deepcopy(dataset)\n",
    "    #drop the columns for further iterations\n",
    "    dataset.drop(dataset.iloc[:, features:], axis = 1, inplace = True)\n",
    "    \n",
    "    return data_dict,dataset,final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "K-means update function\n",
    "\n",
    "Functionality: Based on the dataframe returned from the assignment function, the mean of the datapoints for each cluster is calculated and theie centroids for are updated.\n",
    "\n",
    "Return value: The new updated centroids are returned which is used in the kmeans_assign function to reassign clusters\n",
    "\n",
    "'''\n",
    "def kmeans_update(data_dict,features,centroids):\n",
    "\n",
    "    #create a dictionary with the mean values of the datapoints at cluster level\n",
    "    avgDict = {}\n",
    "    \n",
    "    for k,v in data_dict.items():\n",
    "        avgDict[k] = np.mean(v, axis = 0)\n",
    "\n",
    "    #sort the dict to assign the centroids at their corresponding index\n",
    "    avg_dict = sorted(avgDict.items())\n",
    "\n",
    "    #update the centroids array with the mean centriod of each cluster\n",
    "    for ad in range(len(avg_dict)):\n",
    "        if str(ad) in avg_dict[ad]:\n",
    "            centroids[ad] = [avg_dict[ad][1]]\n",
    " \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Silhouette method\n",
    "\n",
    "Silhouette method is used to find optimum number of clusters for a dataset based on Silhouette Coefficient (SC).\n",
    "\n",
    "For each point i in the dataset ai, bi, and corresponding si is calculated.\n",
    "\n",
    "ai is calculated as the mean of the intra cluster distance between a point \"i\" to all the points in that cluster. \n",
    "\n",
    "        ai = (1/|Ci| - 1) * Σ d(i,j)\n",
    "\n",
    "    where i ≠ j, j ∈ Ci\n",
    "    Ci = cluster\n",
    "    |Ci| = length of that cluster\n",
    "    |Ci| - 1 is done as distance between i and i should not be included.\n",
    "\n",
    "bi is the mean inter cluster distance of a point \"i\" to all points in its neignboring clusters. The minimum distance of these bi is the final bi for the point \"i\".\n",
    "\n",
    "        bi = min (1 / |Ck|) * Σ d(i,j)\n",
    "\n",
    "    where j ∈ Ck\n",
    "    |Ck| = length of Ck cluster\n",
    "\n",
    "si is the silhouette value for each point \"i\".\n",
    "\n",
    "        si = (bi - ai) / max(bi,ai)     if |Ci| > 1\n",
    "    and\n",
    "        si = 0 if |Ci| = 1\n",
    "\n",
    "s(k) is the mean of si over all the points for given number of clusters\n",
    "\n",
    "Silhouette Coeffecient is the max of sk for the given cluster.\n",
    "\n",
    "    SC = max(sk)\n",
    "\n",
    "For a dataset with n points and predefined k clusters, SC is calculated for each k. The optimum k is decided corresponding to max SC.\n",
    "\n",
    "Return value: The mean si value is returned for a given cluster number.\n",
    "\n",
    "'''\n",
    "def silhoutte(data_dict,features):\n",
    "    \n",
    "    #declare arrays for ai, bi, si\n",
    "    ai = []\n",
    "    max_int = float('inf')\n",
    "    bi = []\n",
    "    si = []\n",
    "    \n",
    "    for key,value in data_dict.items():\n",
    "        \n",
    "            #calculate ai\n",
    "            for point in data_dict[key]:\n",
    "                ai_dist = 0\n",
    "                bi_dist = 0\n",
    "                \n",
    "                for sec_point in data_dict[key]:\n",
    "                    if point != sec_point:                        \n",
    "                         #intra cluster distance between point \"i\" to all points\n",
    "                        ai_dist += distance.cdist([point],[sec_point],metric = \"euclidean\")\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                if len(data_dict[key]) == 1:\n",
    "                    mean_ai = [ai_dist]\n",
    "                else:\n",
    "                    mean_ai = ai_dist / (len(data_dict[key]) - 1)\n",
    "                \n",
    "                ai.append(mean_ai)\n",
    "\n",
    "                #calculate bi\n",
    "                for ky,val in data_dict.items():\n",
    "                    if ky != key:\n",
    "                        #inter cluster distance between point \"i\" to all points in its neighboring cluster\n",
    "                        bi_dist = distance.cdist([point],val,metric = \"euclidean\")\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    #mean bi for a cluster\n",
    "                    cluster_bi_mean = np.mean(bi_dist)\n",
    "\n",
    "                    #find the min bi between for point \"i\"\n",
    "                    if cluster_bi_mean < max_int:\n",
    "                        max_int = cluster_bi_mean\n",
    "                \n",
    "                bi.append(max_int)\n",
    "\n",
    "    #calculate si\n",
    "    for x in range(len(ai)):\n",
    "        if ai[x] == bi[x] or ai[x] == 0:\n",
    "            si_value = 0\n",
    "            si.append(si_value)\n",
    "        else:\n",
    "            si_value = (bi[x] - ai[x]) / max(bi[x],ai[x])\n",
    "            si.append(si_value)\n",
    "    \n",
    "    #calculate mean si for a cluster\n",
    "    #mean_si = sum(si) / len(si)\n",
    "    mean_si = np.mean(si)\n",
    "\n",
    "    return mean_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "K-means function\n",
    "\n",
    "Functionality of K-means clustering:\n",
    "Step 1: Based on the pre-defined number of clusters initialise random centroids using Canopy method \n",
    "Step 2: Assign the datapoints to different clusters by finding the minimum distance between the point and centroid\n",
    "Step 3: Based on the assignment, update the centroids list with the mean of the data points in each cluster\n",
    "Step 4: Iterate assign and update methods till the centroids dont change. This defines the convergence of K-means algorithm\n",
    "Step 5: Calculate Silhouette Coeffcient for every cluster to find the optimum cluster for a dataset\n",
    "\n",
    "Return value: Dataframe and dictionary with points assigned to their respective clusters and SC values for predefined number of clusters\n",
    "\n",
    "'''\n",
    "\n",
    "def k_means(dataset,features,data,clusters):\n",
    "    \n",
    "    initial_centroid = []   \n",
    "    i = 1\n",
    "    distances = 0\n",
    "    centroids = []\n",
    "    #sil_arr = []\n",
    "    #max_si = 0\n",
    "    sc = []\n",
    "    \n",
    "    for clu in clusters:\n",
    "\n",
    "        #call the canopy function\n",
    "        centroids = canopy(data,features,clu)\n",
    "        \n",
    "        #find the best centroid by calling kmeans_assign and kmeans_update functions iteratively\n",
    "        prev_centroid = [None] * len(centroids)\n",
    "        cur_centroid = centroids\n",
    "\n",
    "        while True:\n",
    "\n",
    "            data_dict,dataset,final_dataset = kmeans_assign(cur_centroid,data,dataset,features)\n",
    "            #make a copy of current centroid to previous centroid\n",
    "            prev_centroid = copy.deepcopy(cur_centroid)\n",
    "            #update current centroid from the k_means update function\n",
    "            cur_centroid = kmeans_update(data_dict,features,cur_centroid)\n",
    "            #find the error between prev_centroid and cur_centroid\n",
    "            error = (np.sum(prev_centroid,axis = 0) - np.sum(cur_centroid,axis = 0))\n",
    "\n",
    "            if error.all() == 0:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        #call the kmeans_assign to complete the final assignment with best centroids\n",
    "        data_dict,final_data_pts,final_dataset = kmeans_assign(cur_centroid,data,dataset,features)\n",
    "        \n",
    "        #call silhoutte method\n",
    "        #mean si value returned from silhouette\n",
    "        sil = silhoutte(data_dict,features)\n",
    "        #print(sil)\n",
    "        #sil = np.asscalar(sil)\n",
    "\n",
    "        #intialise initial_centroid and i back to original values for next cluster iteration\n",
    "        initial_centroid = []\n",
    "        i = 1\n",
    "\n",
    "        #check for cluster number, if k = 1, append sil = 0 to SC\n",
    "        if clu == 1:\n",
    "            sc.append(0)\n",
    "        else:\n",
    "            sc.append(sil)\n",
    "\n",
    "    return data_dict,final_dataset,sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-771.2, -1582.8]\n",
      "[-1582.0, -1582.7]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Test for the data validation\n",
    "\n",
    "The dataset should contain only real numbers and is tested for empty values, alphanumeric and special characters\n",
    "\n",
    "'''\n",
    "\n",
    "if dataset.isnull().values.any() or ~dataset.applymap(np.isreal).all().all():\n",
    "    \n",
    "    exit()\n",
    "\n",
    "else:\n",
    "    \n",
    "    #create a list for the range of clusters\n",
    "    cluster_range = [2**n for n in range(0,6)]\n",
    "    \n",
    "    '''\n",
    "    call the k-means function\n",
    "    kmeans_df = Is the final dataframe with datapoints assigned to their clusters\n",
    "    sil_coeff = array of Silhouette coefficient for each cluster\n",
    "    '''\n",
    "    \n",
    "    kmeans_dict,kmeans_df,sc = k_means(dataset,features,data,cluster_range)\n",
    "\n",
    "    #to find optimum clusters,k\n",
    "    min_sc = 0\n",
    "    max_sc = float('inf')\n",
    "    new_k = 0\n",
    "\n",
    "    #find the cluster number where the sc values first decreases\n",
    "    for sc_val in range(1,len(sc)-1):\n",
    "        if sc[sc_val + 1] > sc[sc_val]:\n",
    "            continue        \n",
    "        else:\n",
    "            new_k = sc_val + 1\n",
    "            break\n",
    "\n",
    "    #create a new clusters range to find optimum k\n",
    "    #if cluster_range[new_k] == 2\n",
    "    new_clu = [clus for clus in range(int(cluster_range[new_k] / (2**2)),cluster_range[new_k]+1)]\n",
    "\n",
    "    k_means_dict,final_data_pts,sil_coef = k_means(dataset,features,data,new_clu)\n",
    "\n",
    "    max_sc = max(sil_coef)\n",
    "\n",
    "    #get the optimum k\n",
    "    optimum_k = [new_clu[sil_coef.index(max_sc)]]\n",
    "    #perform k_means on optimum k\n",
    "    final_dict,final_df,sil_coeff = k_means(dataset,features,data,optimum_k)\n",
    "\n",
    "    '''\n",
    "\n",
    "    Anomaly Detection\n",
    "    \n",
    "    1. The small clusters with less than a threshold: A dictionary with clusters as keys and count of number of points as values is created. A threshold of 1% of the dataset  is\n",
    "       calculated and based on the value count in the dictionary, anomalies are detected.\n",
    "    2. Isolation data points not belong to any cluster: Based on above created dictionary, the clusters with values as 1 as detected as isolation points.\n",
    "    3. A data point belongs to a cluster with more than 2 standard deviations: Mean, SD and Mean ± 2*SD for each dimension is calculated at cluster level and points not in this\n",
    "       range as detected as anomalies.\n",
    "\n",
    "    '''\n",
    "\n",
    "    #create an anomaly array\n",
    "    anomaly_array = []\n",
    "    \n",
    "    threshold = int(0.01 * len(data))\n",
    "\n",
    "    #create array of final dataframe\n",
    "    final_arr = final_df.values\n",
    "\n",
    "    #dictionary with cluster and count of number of points\n",
    "    cluster_len = {}\n",
    "\n",
    "    for s in range(len(final_arr)):\n",
    "        if final_arr[s][-1] in cluster_len:\n",
    "            cluster_len[final_arr[s][-1]] += 1\n",
    "        else:\n",
    "            cluster_len[final_arr[s][-1]] = 1\n",
    "\n",
    "    for clster,len_val in cluster_len.items():\n",
    "        #detect isolation points\n",
    "        if len_val == 1:\n",
    "            anomaly_array.append(final_dict[clster])\n",
    "        #detect clusters with points less than threshold\n",
    "        elif len_val < threshold:\n",
    "            for values in final_dict[clster]:\n",
    "                anomaly_array.append(values)\n",
    "\n",
    "    anamoly_dict = {}\n",
    "    for clu_key,clu_val in final_dict.items():\n",
    "        anamoly_dict[clu_key] = [(np.mean(clu_val,axis = 0) + 2*np.std(clu_val,axis = 0)).tolist(),(np.mean(clu_val,axis = 0) - 2*np.std(clu_val,axis = 0)).tolist()]\n",
    "        \n",
    "    z = 0\n",
    "    for key1 in set(anamoly_dict.keys()) & set(final_dict.keys()):\n",
    "        for value1 in final_dict[key1]:\n",
    "            #check if each point is within the mean ± 2*SD range\n",
    "            if ~(np.less(np.array(value1),np.array(anamoly_dict[key1][z])).any()) and np.greater(np.array(np.array(value1)),anamoly_dict[key1][z+1]).all():\n",
    "                anomaly_array.append(value1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    anomalies_array = [[np.round(float(i), 2) for i in nested] for nested in anomaly_array]\n",
    "    \n",
    "    if len(anomalies_array) == 0:\n",
    "        print(\"No anomalies\")\n",
    "    else:\n",
    "        for anomaly in anomalies_array:\n",
    "            print(anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
